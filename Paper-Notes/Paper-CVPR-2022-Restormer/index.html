<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>【CVPR 2022】Restormer：Efficient Transformer for High-Resolution Image Restoration | Modesty</title><meta name="keywords" content="Deep Learning,论文笔记,Computer Vision,图像恢复,Image Restoration"><meta name="author" content="HRui"><meta name="copyright" content="HRui"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本文提出一种low-level视觉新Transformer模型：Restormer，在多个图像恢复任务上取得了最先进的结果，包括图像去雨、单图像运动去模糊、散焦去模糊（单图像和双像素数据）和图像去噪等，优于SwinIR、IPT等网络。">
<meta property="og:type" content="article">
<meta property="og:title" content="【CVPR 2022】Restormer：Efficient Transformer for High-Resolution Image Restoration">
<meta property="og:url" content="https://hr-insist.github.io/Paper-Notes/Paper-CVPR-2022-Restormer/index.html">
<meta property="og:site_name" content="Modesty">
<meta property="og:description" content="本文提出一种low-level视觉新Transformer模型：Restormer，在多个图像恢复任务上取得了最先进的结果，包括图像去雨、单图像运动去模糊、散焦去模糊（单图像和双像素数据）和图像去噪等，优于SwinIR、IPT等网络。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hr-insist.github.io/Paper-Notes/Paper-CVPR-2022-Restormer/cover.png">
<meta property="article:published_time" content="2022-10-28T11:26:50.000Z">
<meta property="article:modified_time" content="2024-07-23T01:54:22.007Z">
<meta property="article:author" content="HRui">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="论文笔记">
<meta property="article:tag" content="Computer Vision">
<meta property="article:tag" content="图像恢复">
<meta property="article:tag" content="Image Restoration">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hr-insist.github.io/Paper-Notes/Paper-CVPR-2022-Restormer/cover.png"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://hr-insist.github.io/Paper-Notes/Paper-CVPR-2022-Restormer/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="mjzhXTFsn0W2t12m_s8M0rABMWYSTK2H8cwOgjJ0cww"/><meta name="baidu-site-verification" content="code-Tm06TRSgOZ"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【CVPR 2022】Restormer：Efficient Transformer for High-Resolution Image Restoration',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-23 09:54:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mycss.css"><link rel="stylesheet" href="/css/math.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Contents</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/games/2048%E5%B0%8F%E6%B8%B8%E6%88%8F/"><i class="fa-fw fas fa-chess-board"></i><span> 2 0 4 8</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/photos/"><i class="fa-fw fas fa-image"></i><span> Photos</span></a></div><div class="menus_item"><a class="site-page" href="/log/"><i class="fa-fw fas fa-clock"></i><span> Log</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/Paper-Notes/Paper-CVPR-2022-Restormer/Restormer.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Modesty</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Contents</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/games/2048%E5%B0%8F%E6%B8%B8%E6%88%8F/"><i class="fa-fw fas fa-chess-board"></i><span> 2 0 4 8</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/photos/"><i class="fa-fw fas fa-image"></i><span> Photos</span></a></div><div class="menus_item"><a class="site-page" href="/log/"><i class="fa-fw fas fa-clock"></i><span> Log</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【CVPR 2022】Restormer：Efficient Transformer for High-Resolution Image Restoration</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-28T11:26:50.000Z" title="发表于 2022-10-28 19:26:50">2022-10-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-23T01:54:22.007Z" title="更新于 2024-07-23 09:54:22">2024-07-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper-Notes/">Paper Notes</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【CVPR 2022】Restormer：Efficient Transformer for High-Resolution Image Restoration"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><div class="note info flat"><ul>
<li>Restormer: Efficient Transformer for High-Resolution Image Restoration是2022年发表在CVPR上的一篇关于Image Restoration的论文，在多个图像恢复任务上取得了最先进的结果，包括<strong>图像去雨、单图像运动去模糊、散焦去模糊（单图像和双像素数据）和图像去噪</strong>等。</li>
<li>论文下载地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.09881">https://arxiv.org/abs/2111.09881</a></li>
<li>代码托管：<a target="_blank" rel="noopener" href="https://github.com/swz30/Restormer">https://github.com/swz30/Restormer</a></li>
</ul>
</div>
<h1 id="一、Abstract"><a href="#一、Abstract" class="headerlink" title="一、Abstract"></a>一、Abstract</h1><p>CNNs 对于从大规模数据中学习泛化的图像先验知识(priors)方面表现良好，因此已广泛用于图像复原和相关任务中。而就在最近，另一类神经网络架构 Transformers 在自然语言和 high-level 视觉任务上表现出了显著的性能提升。虽然 Transformer 模型减轻了 CNN 的缺点（即有限的感受野和对输入内容的不适应），但<strong>其计算复杂度随空间分辨率成二次方增长</strong>，因此无法应用于大多数涉及高分辨率图像的图像复原任务中。在这项工作中，作者通过在构建块（多头注意力和前馈网络）中进行几个关键设计来提出一种高效的 Transformer 模型，以便它<strong>可以捕获远程像素交互</strong>，同时仍然适用于大图像。该模型取名为 Restoration Transformer (Restormer)，在多个图像复原任务上取得了SOTA结果，包括图像去雨、单图像运动去模糊、散焦去模糊（单图像和双像素数据）和图像去噪（高斯灰度/彩色去噪和真实图像去噪）。</p>
<h1 id="二、Introduction"><a href="#二、Introduction" class="headerlink" title="二、Introduction"></a>二、Introduction</h1><p>图像复原 (Image restoration) 是通过从退化（degraded）的图像输入中去除退化（例如，噪声、模糊、雨滴）来重建 (reconstructing) 高质量图像的任务。由于卷积神经网络 (CNN) 在从大规模数据中学习<strong>可泛化先验知识</strong>方面表现良好，因此它们已成为首选的复原方法。</p>
<p>在 CNN 中，其基本操作是“卷积”，它为 CNN 提供了<strong>局部连接和平移不变性</strong>的特性。虽然这些特性为 CNN 带来了效率和泛化性，但它们也产生了两个主要问题：(a) <strong>卷积算子的感受野有限</strong>，无法对长距离像素依赖性进行建模；(b) 卷积 filters 在推理时具有<strong>静态权重</strong>，<strong>不能灵活地适应输入内容</strong>。为了解决上述缺点，一种更强大和动态的替代方案是<strong>自注意力 (SA) 机制</strong>，它通过所有其他位置的加权和来计算给定像素的输出特征。虽然SA在捕获远程像素交互方面非常有效，但其复杂度随空间分辨率呈二次方增长，因此无法应用于高分辨率图像。</p>
<p>作者从自注意力（SA）、前馈网络（FN）对 transformer 进行改进，并使用一种渐进性学习策略来训练模型。本文的主要贡献可以总结如下：</p>
<ul>
<li><strong>提出了Restormer，一种编码-解码结构的Transformer</strong>，用于在<strong>高分辨率图像</strong>上进行多尺度局部/全局表示学习，而不将它们分解成局部窗口，从而利用遥远的图像上下文。</li>
<li>提出了一种<strong>使用深度卷积的多头转置注意模块</strong>(multi-Dconv head transposed attention, MDTA)，它能够聚合局部和非局部像素交互，并且足够有效地处理高分辨率图像。</li>
<li>一种新的<strong>使用深度卷积的门控前馈网络</strong>(Gated-Dconv feed-forward network, GDFN)，它执行受控的特征转换，即抑制信息量较少的特征，只允许有用的信息进一步通过网络层次结构。</li>
</ul>
<h1 id="三、Method"><a href="#三、Method" class="headerlink" title="三、Method"></a>三、Method</h1><p>我们的主要目标是开发一个能够处理高分辨率图像的高效Transformer模型，用于恢复任务。为了缓解计算瓶颈，我们将关键设计引入到多头SA层和比单尺度网络计算需求更小的多尺度层次模块。</p>
<img src="/Paper-Notes/Paper-CVPR-2022-Restormer/Restormer.png" class="" title="Restormer">
<p>本文所提方法的整体架构如上图所示。其整体执行流程为：给定降质图像的图像 $I \in \mathbb{R}^{H \times W \times 3} $，Restormer 首先使用一个卷积去获得低级特征嵌入 $F_0 \in \mathbb{R}^{H \times W \times C}$ ，其中 $H \times W$ 表示空间维度而 $C$ 表示通道数。接下来，这些浅层特征 $F_0$ 通过一个 4 级对称的 encoder-decoder 并且转化为深层特征 $F_d \in \mathbb{R}^{H \times W \times 2C}$ 。每一级encoder-decoder都包含多个 Transformer block，block的数量从小到大逐渐增加，以保持效率。从高分辨率输入开始，编码器分层减少空间大小，同时扩展通道容量。解码器把低分辨率的潜在特征$F_l \in \mathbb{R}^{\frac H8 \times \frac W8 \times 8C}$  作为输入并渐进式地恢复出高分辨率表示。对于特征下采样和上采样，作者分别应用pixel-unshuffle和pixel-shuffle操作。为了帮助恢复过程，<strong>编码器的特征通过跳过连接 (skip connection)与解码器的特征串联在一起</strong>。串联操作之后是 $1×1$ 卷积，以减少所有层级的通道数量（减半），顶部的层级除外。在第1级，作者让 Transformer 块将编码器的低级图像特征与解码器的高级特征聚合在一起。这有助于在恢复的图像中保留精细的结构和纹理细节。接下来，在高空间分辨率下操作的再精细阶段，深层特征 $F_d$ 被进一步丰富。我们可以在实验部分中看到，这些设计选择提高了图像质量。最后，对细化后的特征采用卷积层生成残差图像 $R \in \mathbb{R}^{H \times W \times 3} $，其中添加降质的图像以获得复原的图像: $\hat{I} = I + R $。</p>
<h2 id="3-1-多头-深度可分离卷积-转置注意（Multi-Dconv-Head-Transposed-Attention）"><a href="#3-1-多头-深度可分离卷积-转置注意（Multi-Dconv-Head-Transposed-Attention）" class="headerlink" title="3.1 多头 深度可分离卷积 转置注意（Multi-Dconv Head Transposed Attention）"></a>3.1 多头 深度可分离卷积 转置注意（Multi-Dconv Head Transposed Attention）</h2><p>Transformers 中的主要计算开销来自于自注意力（SA）层。 在传统的 SA 中，key-query点积交互的<strong>时间和内存复杂度与输入的空间分辨率成二次方增长</strong>，即对于 $W×H$ 像素的图像，其复杂度为 $O(W^2H^2)$ 。</p>
<ul>
<li>10x10的输入图片  $\Rightarrow$  Attention map size = 100 x 100 pixels</li>
<li>100x100的输入图片  $\Rightarrow$ Attention map size = 10,000 x 10,000 pixels</li>
<li>1000x1000的输入图片 $\Rightarrow$ Map size = 1,000,000 x 1,000,000 pixels</li>
</ul>
<p>因此，将 SA 应用于经常涉及高分辨率图像的大多数图像复原任务是不可行的。 为了缓解这个问题，<strong>作者提出了具有线性复杂度的 MDTA</strong>，如上图 2（a）所示。 <strong>关键因素是跨通道而不是空间维度应用 SA，即计算跨通道的交叉协方差来生成隐式地编码全局上下文的注意力图</strong>。 作为 MDTA 的另一个重要组成部分，作者引入了深度卷积，它在计算特征协方差来产生全局注意力图之前强调局部上下文。</p>
<img src="/Paper-Notes/Paper-CVPR-2022-Restormer/MDTA.png" class="" title="MDTA">
<p>在获得一个层归一化张量 $Y\in \mathbb{R}^{\widehat{H} \times \widehat{W} \times \widehat{C} }$ 之后，MDTA 首先前向传播生成 query（<strong>Q</strong>）、key（<strong>K</strong>）和 value（<strong>V</strong>），这丰富了局部上下文。 它是通过应用 1×1 卷积来聚合像素别和跨通道的上下文，并应用 $3\times3$ 深度卷积来编码通道级空间上下文来实现的，产生 $Q=W_p^QW_d^QY$ ， $K=W_p^KW_d^KY$ 和 $V=W_p^VW_d^VY$ 。其中， $W_p^{(·)}$ 是 $1×1$ 逐点卷积（<strong>point-wise convolution</strong>）， $W_d^{(·)}$ 是 $3 \times 3$ 的深度卷积（<strong>depth-wise convolution</strong>）。</p>
<ul>
<li>逐点卷积 + 深度卷积 = <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/453434386">深度可分离卷积</a></li>
</ul>
<p>作者在网络中使用无偏置卷积层。接下来，作者重塑 query 和 key 投影，使得它们的点积交互生成一个大小为 $R^{\widehat{C} \times \widehat{C} }$ 的转置注意力图 A ，而不是大小为 $R^{\widehat{H}\widehat{W} \times \widehat{H}\widehat{W} }$ 的巨大的常规的注意力图。 总的来说，MDTA 过程被定义为：</p>
<script type="math/tex; mode=display">
\widehat{X} = W_p \cdot Attention(\widehat{Q},\widehat{K},\widehat{V}) + V</script><script type="math/tex; mode=display">
Attention(\widehat{Q},\widehat{K},\widehat{V}) = \widehat{V} \cdot Softmax(\widehat{K} \cdot \widehat{Q} / \alpha)</script><p>其中 $X$ 和 $\widehat{X}$ 是输入和输出特征图；矩阵 $\widehat{Q} \in \mathbb{R}^{\widehat{H}\widehat{W} \times \widehat{C} }$ ，$\widehat{K} \in \mathbb{R}^{\widehat{C} \times \widehat{H}\widehat{W} }$  和 $\widehat{V} \in \mathbb{R}^{\widehat{H}\widehat{W} \times \widehat{C} }$  是从原始大小 $\mathbb{R}^{\widehat{H} \times \widehat{W} \times \widehat{C} }$对张量进行变形后获得的。这里， $\alpha$ 是一个可学习的缩放参数，用于在应用 softmax 函数之前控制 $\widehat{K}$ 和 $\widehat{Q}$ 的点积的大小。 与传统的多头 SA 相似，作者将通道数量划分为“head”，然后并行学习单独的注意力图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Multi-DConv Head Transposed Self-Attention (MDTA)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, bias</span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        self.num_heads = num_heads  <span class="comment"># 注意力头的个数</span></span><br><span class="line">        self.temperature = nn.Parameter(torch.ones(num_heads, <span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># 可学习系数</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1*1 升维</span></span><br><span class="line">        self.qkv = nn.Conv2d(dim, dim*<span class="number">3</span>, kernel_size=<span class="number">1</span>, bias=bias)</span><br><span class="line">        <span class="comment"># 3*3 分组卷积</span></span><br><span class="line">        self.qkv_dwconv = nn.Conv2d(dim*<span class="number">3</span>, dim*<span class="number">3</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=dim*<span class="number">3</span>, bias=bias)</span><br><span class="line">        <span class="comment"># 1*1 卷积</span></span><br><span class="line">        self.project_out = nn.Conv2d(dim, dim, kernel_size=<span class="number">1</span>, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b,c,h,w = x.shape  <span class="comment"># 输入的结构 batch 数，通道数和高宽</span></span><br><span class="line"></span><br><span class="line">        qkv = self.qkv_dwconv(self.qkv(x))</span><br><span class="line">        q,k,v = qkv.chunk(<span class="number">3</span>, dim=<span class="number">1</span>)  <span class="comment">#  第 1 个维度方向切分成 3 块</span></span><br><span class="line">        <span class="comment"># 改变 q, k, v 的结构为 b head c (h w)，将每个二维 plane 展平</span></span><br><span class="line">        q = rearrange(q, <span class="string">&#x27;b (head c) h w -&gt; b head c (h w)&#x27;</span>, head=self.num_heads)</span><br><span class="line">        k = rearrange(k, <span class="string">&#x27;b (head c) h w -&gt; b head c (h w)&#x27;</span>, head=self.num_heads)</span><br><span class="line">        v = rearrange(v, <span class="string">&#x27;b (head c) h w -&gt; b head c (h w)&#x27;</span>, head=self.num_heads)</span><br><span class="line"></span><br><span class="line">        q = torch.nn.functional.normalize(q, dim=-<span class="number">1</span>)  <span class="comment"># C 维度标准化，这里的 C 与通道维度略有不同</span></span><br><span class="line">        k = torch.nn.functional.normalize(k, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) * self.temperature</span><br><span class="line">        attn = attn.softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        out = (attn @ v)  <span class="comment"># 注意力图(严格来说不算图)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将展平后的注意力图恢复</span></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b head c (h w) -&gt; b (head c) h w&#x27;</span>, head=self.num_heads, h=h, w=w)</span><br><span class="line">        <span class="comment"># 真正的注意力图</span></span><br><span class="line">        out = self.project_out(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="3-2-门控深度可分离卷积前馈网络（Gated-Dconv-Feed-Forward-Network）"><a href="#3-2-门控深度可分离卷积前馈网络（Gated-Dconv-Feed-Forward-Network）" class="headerlink" title="3.2 门控深度可分离卷积前馈网络（Gated-Dconv Feed-Forward Network）"></a>3.2 门控深度可分离卷积前馈网络（Gated-Dconv Feed-Forward Network）</h2><p>为了转换特征，常规的前馈网络 (FN) 分别且一致地对每个像素位置进行操作。 它使用两个 1×1 卷积，一个是扩展特征通道（通常因子 $\gamma=4$ ），第二个是将通道减少回原始的输入维度。 在隐藏层中应用了非线性。 在这项工作中，<strong>作者提出了 FN 中的两个基本修改来改进表示学习：（1）门控机制，以及（2）深度卷积</strong>。所提出的 GDFN 的架构如下图右所示。 </p>
<img src="/Paper-Notes/Paper-CVPR-2022-Restormer/GDFN.png" class="" title="GDFN">
<p>门控机制被形式化为线性变换层的两条平行路径的元素乘积，其中之一被 GELU 非线性激活。 与在 MDTA 中一样，作者还在 GDFN 中包含深度卷积来编码来自空间相邻像素位置的信息，这对于学习局部图像结构以进行有效的图像复原很有用。 给定一个输入张量 $X \in \mathbb{R}^{\widehat{H} \times \widehat{W} \times \widehat{C} }$ ，GDFN 被形式化为如下形式：</p>
<script type="math/tex; mode=display">
\widehat{X} = W_p^0 \cdot Gating(X) + X</script><script type="math/tex; mode=display">
Gating(X) = \phi(W_d^1W_p^1(LN(X))) \odot (W_d^2W_p^2(LN(X)))</script><p>其中， $\odot$ 表示逐元素乘法， $\phi$ 表示 GELU 非线性激活函数，LN 是层归一化。 总体而言，GDFN 控制着通过 pipeline 中各个层级的信息流，从而允许每个层级专注于与其它层级互补的细节。也就是说，与 MDTA（专注于用上下文信息丰富特征）相比，GDFN 承担者一个独特的角色。 由于与常规 FN 相比，所提出的 GDFN 执行更多操作，因此作者降低了扩展率 $\gamma$ 以具有相似的参数和计算负担。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Gated-Dconv Feed-Forward Network (GDFN)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, ffn_expansion_factor, bias</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeedForward, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 隐藏层特征维度等于输入维度乘以扩张因子</span></span><br><span class="line">        hidden_features = <span class="built_in">int</span>(dim*ffn_expansion_factor)</span><br><span class="line">        <span class="comment"># 1*1 升维</span></span><br><span class="line">        self.project_in = nn.Conv2d(dim, hidden_features*<span class="number">2</span>, kernel_size=<span class="number">1</span>, bias=bias)</span><br><span class="line">        <span class="comment"># 3*3 分组卷积</span></span><br><span class="line">        self.dwconv = nn.Conv2d(hidden_features*<span class="number">2</span>, hidden_features*<span class="number">2</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, groups=hidden_features*<span class="number">2</span>, bias=bias)</span><br><span class="line">        <span class="comment"># 1*1 降维</span></span><br><span class="line">        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=<span class="number">1</span>, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.project_in(x)</span><br><span class="line">        x1, x2 = self.dwconv(x).chunk(<span class="number">2</span>, dim=<span class="number">1</span>)  <span class="comment"># 第 1 个维度方向切分成 2 块</span></span><br><span class="line">        x = F.gelu(x1) * x2  <span class="comment"># gelu 相当于 relu+dropout</span></span><br><span class="line">        x = self.project_out(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="3-3-渐进式学习（Progressive-Learning）"><a href="#3-3-渐进式学习（Progressive-Learning）" class="headerlink" title="3.3 渐进式学习（Progressive Learning）"></a>3.3 渐进式学习（Progressive Learning）</h2><p>在裁剪后的小 patch 上训练 Transformer 模型可能不会对全局的图像统计信息进行编码，从而在测试时的全分辨率图像上提供次优的性能。为此，作者<strong>使用渐进式学习，其中网络在早期阶段在较小的图像 patch 上进行训练，在后期的训练阶段中逐渐增大。</strong>通过渐进式学习在混合大小的 patch 上训练的模型在测试时表现出增强的性能，其中图像可以具备不同的分辨率（这是图像复原中的常见情况）。渐进式学习策略的行为方式类似于课程学习过程：网络从简单的任务开始，逐渐转向学习更复杂的任务（需要保留精细的图像结构/纹理）。由于对大 patch 的训练需要花费更长的时间，因此作者会<strong>随着 patch 大小的增加而减小batch大小</strong>，以保持每个优化步骤的时间与固定 patch 训练相似。</p>
<h1 id="四、Experiments-and-Analysis"><a href="#四、Experiments-and-Analysis" class="headerlink" title="四、Experiments and Analysis"></a>四、Experiments and Analysis</h1><p>论文对关于实验是如何做的细节描述的很简单，只详细介绍了在Image Restoration的四个方面的实验对比。</p>
<p>论文的消融实验可以看看:</p>
<ul>
<li>深度可分离可以进一步提升模型性能；</li>
<li>encoder-decoder的第一阶段只concate不通过卷积进行通道数减少的融合。第一阶段浅层特征往往包含一些边缘等结构信息，有利于low-level任务。</li>
<li>Progressive学习机制指标更好；</li>
<li>深而窄的模型比宽而浅的模型更好；</li>
</ul>
<h1 id="五、Conclusion"><a href="#五、Conclusion" class="headerlink" title="五、Conclusion"></a>五、Conclusion</h1><p><strong>Restormer：</strong>处理高分辨率图像<strong>方面具有计算效率</strong>。<br><strong>MDTA：</strong>（跨通道而不是空间维度，SA，进行局部与非局部相关像素聚合，建模全局上下文+线性复杂度）<br><strong>GDFN：</strong>引入了<strong>门控机制</strong>控制信息流动, 进而使得每层聚焦于不同的细节信息。抑制低信息特征，仅保留有用信息。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://hr-insist.github.io">HRui</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://hr-insist.github.io/Paper-Notes/Paper-CVPR-2022-Restormer/">https://hr-insist.github.io/Paper-Notes/Paper-CVPR-2022-Restormer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://hr-insist.github.io" target="_blank">Modesty</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><a class="post-meta__tags" href="/tags/Computer-Vision/">Computer Vision</a><a class="post-meta__tags" href="/tags/%E5%9B%BE%E5%83%8F%E6%81%A2%E5%A4%8D/">图像恢复</a><a class="post-meta__tags" href="/tags/Image-Restoration/">Image Restoration</a></div><div class="post_share"><div class="social-share" data-image="/Paper-Notes/Paper-CVPR-2022-Restormer/cover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Paper-Notes/Paper-CVPR-2021-Uformer/"><img class="prev-cover" src="/Paper-Notes/Paper-CVPR-2021-Uformer/Uformer.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">论文笔记Uformer：A General U-Shaped Transformer for Image Restoration</div></div></a></div><div class="next-post pull-right"><a href="/Git/%E5%B8%B8%E7%94%A8Git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/"><img class="next-cover" src="/Git/%E5%B8%B8%E7%94%A8Git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/cover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">常用 Git 命令清单</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/Paper-Notes/Paper-CVPR-2021-MPRnet/" title="论文阅读 Multi-Stage Progressive Image Restoration"><img class="cover" src="/Paper-Notes/Paper-CVPR-2021-MPRnet/MPRnet.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-20</div><div class="title">论文阅读 Multi-Stage Progressive Image Restoration</div></div></a></div><div><a href="/Paper-Notes/Paper-CVPR-2021-Uformer/" title="论文笔记Uformer：A General U-Shaped Transformer for Image Restoration"><img class="cover" src="/Paper-Notes/Paper-CVPR-2021-Uformer/Uformer.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-30</div><div class="title">论文笔记Uformer：A General U-Shaped Transformer for Image Restoration</div></div></a></div><div><a href="/Paper-Notes/Paper-ICCV-2021-Swin_Transformer/" title="论文阅读 Swin Transformer"><img class="cover" src="/Paper-Notes/Paper-ICCV-2021-Swin_Transformer/cover.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-19</div><div class="title">论文阅读 Swin Transformer</div></div></a></div><div><a href="/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/" title="论文阅读 Vision Transformer (ViT)"><img class="cover" src="/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/ViT.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-18</div><div class="title">论文阅读 Vision Transformer (ViT)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HRui</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/HR-Insist"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80%E3%80%81Abstract"><span class="toc-text">一、Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Introduction"><span class="toc-text">二、Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89%E3%80%81Method"><span class="toc-text">三、Method</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E5%A4%9A%E5%A4%B4-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF-%E8%BD%AC%E7%BD%AE%E6%B3%A8%E6%84%8F%EF%BC%88Multi-Dconv-Head-Transposed-Attention%EF%BC%89"><span class="toc-text">3.1 多头 深度可分离卷积 转置注意（Multi-Dconv Head Transposed Attention）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E9%97%A8%E6%8E%A7%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%EF%BC%88Gated-Dconv-Feed-Forward-Network%EF%BC%89"><span class="toc-text">3.2 门控深度可分离卷积前馈网络（Gated-Dconv Feed-Forward Network）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E6%B8%90%E8%BF%9B%E5%BC%8F%E5%AD%A6%E4%B9%A0%EF%BC%88Progressive-Learning%EF%BC%89"><span class="toc-text">3.3 渐进式学习（Progressive Learning）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Experiments-and-Analysis"><span class="toc-text">四、Experiments and Analysis</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81Conclusion"><span class="toc-text">五、Conclusion</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/Java/Dubbo%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C/" title="Dubbo参数校验"><img src="/Java/Dubbo%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C/cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Dubbo参数校验"/></a><div class="content"><a class="title" href="/Java/Dubbo%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C/" title="Dubbo参数校验">Dubbo参数校验</a><time datetime="2024-07-23T01:58:40.000Z" title="发表于 2024-07-23 09:58:40">2024-07-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Java/Dubbo%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/" title="Dubbo异常处理"><img src="/Java/Dubbo%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Dubbo异常处理"/></a><div class="content"><a class="title" href="/Java/Dubbo%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/" title="Dubbo异常处理">Dubbo异常处理</a><time datetime="2024-07-22T15:40:58.000Z" title="发表于 2024-07-22 23:40:58">2024-07-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/uncategorized/SpringCloud%E8%B0%B7%E7%B2%92%E5%95%86%E5%9F%8E%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/" title="SpringCloud谷粒商城中遇到的问题"><img src="/images/default_cover4.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SpringCloud谷粒商城中遇到的问题"/></a><div class="content"><a class="title" href="/uncategorized/SpringCloud%E8%B0%B7%E7%B2%92%E5%95%86%E5%9F%8E%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/" title="SpringCloud谷粒商城中遇到的问题">SpringCloud谷粒商城中遇到的问题</a><time datetime="2023-10-15T06:27:11.000Z" title="发表于 2023-10-15 14:27:11">2023-10-15</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By HRui</div><div class="footer_custom_text"><p> <a style="margin-inline:5px"target="_blank" href="https://hexo.io/"> <img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为 Hexo" alt="HEXO"> </a> <a style="margin-inline:5px"target="_blank" href="https://butterfly.js.org/"> <img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用 Butterfly" alt="Butterfly"> </a> <a style="margin-inline:5px"target="_blank" href="https://github.com/"> <img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由 GitHub 托管" alt="GitHub"> </a> <a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"> <img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" alt="img" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"> </a> </p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>