<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>论文阅读 Vision Transformer (ViT) | Modesty</title><meta name="keywords" content="Vision Transformer, ViT"><meta name="author" content="HRui"><meta name="copyright" content="HRui"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ViT将CV和NLP结合起来，对原始图片进行分块，展平成序列，输入进原始Transformer模型的编码器部分，最后接入一个全连接层对图片进行分类。">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读 Vision Transformer (ViT)">
<meta property="og:url" content="https://hr-insist.github.io/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/index.html">
<meta property="og:site_name" content="Modesty">
<meta property="og:description" content="ViT将CV和NLP结合起来，对原始图片进行分块，展平成序列，输入进原始Transformer模型的编码器部分，最后接入一个全连接层对图片进行分类。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hr-insist.github.io/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/ViT.png">
<meta property="article:published_time" content="2022-09-18T07:46:44.000Z">
<meta property="article:modified_time" content="2024-07-23T01:54:22.009Z">
<meta property="article:author" content="HRui">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="论文笔记">
<meta property="article:tag" content="Computer Vision">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hr-insist.github.io/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/ViT.png"><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="https://hr-insist.github.io/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="mjzhXTFsn0W2t12m_s8M0rABMWYSTK2H8cwOgjJ0cww"/><meta name="baidu-site-verification" content="code-Tm06TRSgOZ"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文阅读 Vision Transformer (ViT)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-23 09:54:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mycss.css"><link rel="stylesheet" href="/css/math.css"><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Contents</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/games/2048%E5%B0%8F%E6%B8%B8%E6%88%8F/"><i class="fa-fw fas fa-chess-board"></i><span> 2 0 4 8</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/photos/"><i class="fa-fw fas fa-image"></i><span> Photos</span></a></div><div class="menus_item"><a class="site-page" href="/log/"><i class="fa-fw fas fa-clock"></i><span> Log</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/ViT.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Modesty</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Contents</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-gamepad"></i><span> Games</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/games/2048%E5%B0%8F%E6%B8%B8%E6%88%8F/"><i class="fa-fw fas fa-chess-board"></i><span> 2 0 4 8</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/photos/"><i class="fa-fw fas fa-image"></i><span> Photos</span></a></div><div class="menus_item"><a class="site-page" href="/log/"><i class="fa-fw fas fa-clock"></i><span> Log</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读 Vision Transformer (ViT)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-18T07:46:44.000Z" title="发表于 2022-09-18 15:46:44">2022-09-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-23T01:54:22.009Z" title="更新于 2024-07-23 09:54:22">2024-07-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper-Notes/">Paper Notes</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文阅读 Vision Transformer (ViT)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><div class="note info flat"><ul>
<li>论文名称：<strong>An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale</strong></li>
<li>论文下载地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></li>
<li>代码托管：<a target="_blank" rel="noopener" href="https://github.com/google-research/vision_transformer">https://github.com/google-research/vision_transformer</a></li>
<li>B站朱毅大神视频讲解：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV15P4y137jb/">https://www.bilibili.com/video/BV15P4y137jb/</a></li>
<li>代码复现：<a target="_blank" rel="noopener" href="https://github.com/HR-Insist/paper-code/tree/master/ViT">https://github.com/HR-Insist/paper-code/tree/master/ViT</a></li>
</ul>
</div>
<div class="note info flat"><p><strong>Vision Transformer</strong>将CV和NLP领域知识结合起来应用于<strong>Image Classification</strong>，<strong>对原始图片进行分块，展平成序列，输入进原始Transformer模型的编码器Encoder部分</strong>，最后接入一个全连接层对图片进行分类。在大型数据集上表现超过了当时SOTA模型。</p>
</div>
<h2 id="一、Abstract"><a href="#一、Abstract" class="headerlink" title="一、Abstract"></a>一、Abstract</h2><p>&emsp;&emsp;虽然 <strong>Transformer</strong> 架构已成为 NLP 任务的事实标准，但它在 CV 中的应用仍然有限。在视觉领域上，attention 要么与卷积网络结合使用，要么用于替换卷积网络的某些组件，同时保持其整体结构。我们证明了这种对 CNNs 的依赖是不必要的，直接应用于图像块序列  (sequences of image patches) 的纯 Transformer 可以很好地执行<strong>图像分类</strong>任务。当对大量数据进行预训练并迁移到多个中小型图像识别基准时 (ImageNet、CIFAR-100、VTAB 等)，与 SOTA 的 CNN 相比，<strong>Vision Transformer (ViT)</strong> 可获得更优异的结果，同时仅需更少的训练资源。</p>
<h2 id="二、Introduction"><a href="#二、Introduction" class="headerlink" title="二、Introduction"></a>二、Introduction</h2><p>&emsp;&emsp;基于<strong>self-attention</strong>的架构，尤其是 <strong>Transformer</strong>，已成为 NLP 中的首选模型。主要方法是<strong>在大型文本语料库上进行预训练，然后在较小而特定于任务的数据集上进行微调</strong>。 由于 Transformers 的计算效率和可扩展性，训练具有超过 100B 个参数的、前所未有的模型成为了可能。随着模型和数据集的增长，仍未表现出饱和迹象。 </p>
<p>&emsp;&emsp;受 NLP 中 Transformer 成功<strong>放缩 (scaling)</strong> 的启发，我们尝试<strong>将标准 Transformer 直接应用于图像，并尽可能减少修改</strong>。为此，<strong>我们将图像拆分为块 (patch)，并将这些图像块的 线性嵌入序列 (the sequence of linear embeddings) 作为 Transformer 的输入。图像块 image patches 的处理方式与 NLP 应用中的标记 tokens (单词 words) 相同</strong>。我们以有监督方式训练图像分类模型。</p>
<p>&emsp;&emsp;当在没有强正则化的中型数据集（如 ImageNet）上进行训练时，这些模型产生的准确率比同等大小的 ResNet 低几个百分点。 这种看似令人沮丧的结果可能是意料之中的：<strong>Transformers 缺乏 CNN 固有的一些归纳偏置 (inductive biases)，例如平移等效性 (translation equivariance) 和局部性 (locality)，因此在数据量不足的情况下训练时不能很好地泛化</strong>。 </p>
<p>&emsp;&emsp;但是，如果模型在更大的数据集 (14M-300M 图像) 上训练，情况就会发生变化。我们发现 <strong>大规模训练胜过归纳偏置 ( large scale training trumps inductive bias)</strong>。我们的 <strong>Vision Transformer (ViT) 在以足够的规模进行预训练并迁移到具有较少数据点的任务时获得了出色结果</strong>。当在公共 ImageNet-21k 数据集或内部 JFT-300M 数据集上进行预训练时，ViT 在多个图像识别基准上接近或击败了最先进的技术。特别是，最佳模型在 ImageNet 上的准确率达到 88.55%，在 ImageNet-RealL 上达到 90.72%，在 CIFAR-100 上达到 94.55%，在 19 个任务的 VTAB 上达到 77.63%。</p>
<h2 id="三、Conclution"><a href="#三、Conclution" class="headerlink" title="三、Conclution"></a>三、Conclution</h2><p>我们已经探索了Transformers 在图像识别中的直接应用。与之前在计算机视觉中使用self-attention的工作不同，除了最初的补丁提取步骤外，我们没有在架构中引入image-specific inductive biases (特定于图像的感应偏差)。相反，我们将图像解释为一个补丁序列，并通过像NLP中使用的标准Transformers  encoder来处理它。这种简单而又可扩展的策略在与大型数据集的预训练相结合时效果出奇地好。因此，Vision Transformer在许多图像分类数据集上符合或超过了技术水平，同时预训练也相对便宜。虽然这些初步结果令人鼓舞，但仍有许多挑战。<strong>一个是</strong>将ViT应用于其他计算机视觉任务，如检测和分割。我们的结果，加上Carion等人（2020）的结果，表明了这种方法的前景。<strong>另一个</strong>挑战是继续探索自我监督的预训练方法。我们的初步实验表明自监督预训练有了很大的改进，但自监督和大规模监督预训练之间仍有很大差距。最后，ViT的进一步扩展可能会导致性能的提高。</p>
<h2 id="四、Related-Work"><a href="#四、Related-Work" class="headerlink" title="四、Related Work"></a>四、Related Work</h2><p>Transformers 是由 Vaswani 等人提出的<strong>机器翻译</strong>方法，并已成为许多 NLP 任务中最先进的方法。基于大型 Transformer 的模型通常在大型语料库上进行预训练，然后根据手头的任务进行微调：<strong>BERT</strong>使用 <strong>去噪自监督</strong> 预训练任务，而 <strong>GPT</strong> 工作线使用 <strong>语言建模</strong> 作为其预训练任务。</p>
<p>应用于图像的简单自注意力要求 <strong>每个像素关注所有其他像素</strong>。由于像素数量的二次方成本，其无法缩放到符合实际的输入尺寸。因此，曾经有研究者尝试过几种近似方法以便于在图像处理中应用 Transformer。Parmar 等人只在每个 query 像素的局部邻域而非全局应用自注意力，这种局部多头点积自注意力块完全可以代替卷积。在另一种工作中，稀疏 Transformer 采用可放缩的全局自注意力，以便适用于图像。衡量注意力的另一种方法是将其应用于大小不同的块中，在极端情况下仅沿单个轴。许多这种特殊的注意力架构在 CV 任务上显示出很好的效果，但是需要在硬件加速器上有效地实现复杂的工程。</p>
<p><strong>与我们最相关的是 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.03584">Cordonnier </a>等人的模型，该模型从输入图像中提取 2×2 大小的块，并在顶部应用完全的自注意力</strong>。该模型与ViT 非常相似，但我们的工作进一步证明了<strong>大规模的预训练使普通的 Transformers 能够与 SOTA 的 CNNs 竞争 (甚至更优)</strong>。此外，Cordonnier 等人使用 2×2 像素的小块，使模型只适用于小分辨率图像，而我们也能处理中分辨率图像。</p>
<p>将 CNN 与自注意力的形式相结合有很多有趣点，例如增强用于图像分类的特征图，或使用自注意力进一步处理CNN 的输出，如用于目标检测、视频处理、图像分类，无监督目标发现，或统一文本视觉任务。</p>
<p>另一个最近的相关模型是图像 GPT (iGPT)，它在降低图像分辨率和颜色空间后对图像像素应用 Transformers。该模型以无监督的方式作为生成模型进行训练，然后可以对结果表示进行微调或线性探测以提高分类性能，在 ImageNet 上达到 72% 的最大精度。</p>
<p>我们的工作增加了在比标准 ImageNet 数据集更大尺度上探索图像识别的论文的数量。使用额外的数据源可以在标准基准上取得 SOTA 的成果。此外，Sun 等人研究了 CNN 性能如何随数据集大小而变化，Kolesnikov、Djolonga 等人从 ImageNet-21k 和JFT-300M 等大规模数据集对 CNN 迁移学习进行了实证研究。我们也关注后两个数据集，但是是训练 Transformers 而非以前工作中使用的基于 ResNet 的模型。</p>
<h2 id="五、Method"><a href="#五、Method" class="headerlink" title="五、Method"></a>五、Method</h2><p>在模型设计中，我们尽可能地遵循原始 Transformer (Vaswani 等, 2017)。 这种有意简单设置的优势在于，可扩展的 NLP Transformer 架构及其高效实现几乎可以开箱即用。</p>
<img src="/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/ViT.png" class="" title="Figure1">
<p>Figure1：模型概述。我们将一张图像切割成一些固定尺寸的图像块，对每个pathc进行线性嵌入，并添加位置嵌入，然后将产生的向量序列输入到标准的Transformer encoder中。为了执行识别任务，我们添加了一个额外可学习的classification token到序列中。Transformer 编码器的插图受Vaswani 的Transformer(Attention is all you need)的启发。</p>
<h3 id="5-1-Patch-Embeddings"><a href="#5-1-Patch-Embeddings" class="headerlink" title="5.1 Patch Embeddings"></a>5.1 Patch Embeddings</h3><p>上图概述了ViT的结构。原始的Transformer接受<strong>一维标记嵌入序列 (Sequence of token embeddings)</strong> 作为输入。为了处理2D图像，我们将图像 ${x \in {R^{H \times W \times C} } }$  reshape为一个展平 (flatten) 的2D块序列，即${x \in {R^{H \times (P^2 \cdot C)} } }$ ，其中 $( H , W )$ 是原图的分辨率，$C$ 是通道数 (RGB图像$C=3$)，$( P , P )$ 是每一个图像块的分辨率，$N = HW/P^2$ 是产生的图像块的个数，同时也作为Transformer的有效输入的序列长度。Transformer在其所有层中使用固定的隐向量 (latent vector) 大小D，因此我们将图像块展平，并使用 <strong>可训练的线性投影 (FC 层)</strong> 将维度 ${P^2 \cdot C}$ 映射为 $D$ 维，同时保持图像块数 $N$ 不变 (等式 1)。我们将这个投影的输出称之为<strong>块嵌入 (patch embeddings)</strong> 。</p>
<ul>
<li>对于图像数据而言，其数据格式为[H, W, C]是三维矩阵明显不是Transformer想要的。所以需要先通过一个Embedding层来对数据做个变换。首先将一张图片按给定大小分成一堆Patches。以ViT-B/16为例，将输入图片(224x224) 按照16x16大小的Patch进行划分，划分后会得到 $(224/16)^2=14^2=196$ 个Patches。接着通过线性映射将每个Patch映射到一维向量中，以ViT-B/16为例，每个Patch数据shape为[16, 16, 3]通过映射得到一个长度为768 ($16 \times 16 \times 3 = 768$) 的向量。此时就得到一个<code>[196, 768]</code> 的嵌入向量。到目前为止就已经成功地将一个vision的问题变成了一个NLP的问题了，输入就是一系列1D的token，而不再是一张2D的图片了。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Image to Patch Embedding &quot;&quot;&quot;</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># (H, W)</span></span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        <span class="comment"># (P, P)</span></span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        <span class="comment"># N = (H // P) * (W // P)</span></span><br><span class="line">        num_patches = (img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]) * (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.num_patches = num_patches</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 可训练的线性投影 - 获取输入嵌入</span></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="comment"># FIXME look at relaxing size constraints</span></span><br><span class="line"> </span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*</span></span><br><span class="line"><span class="string"><span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># (B, C, H, W) -&gt; (B, D, (H//P), (W//P)) -&gt; (B, D, N) -&gt; (B, N, D)</span></span><br><span class="line">        <span class="comment">#   D=embed_dim=768, N=num_patches=(H//P)*(W//P)</span></span><br><span class="line">        <span class="comment">#   torch.flatten(input, start_dim=0, end_dim=-1)  # 形参：展平的起始维度和结束维度    </span></span><br><span class="line">        <span class="comment"># 可见 Patch Embedding 操作 3 步到位</span></span><br><span class="line">        x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="5-2-Learnable-Embedding"><a href="#5-2-Learnable-Embedding" class="headerlink" title="5.2 Learnable Embedding"></a>5.2 Learnable Embedding</h3><p>类似于 BERT 的 class token，此处 <strong>为图像块嵌入序列预设一个可学习的嵌入 (Learnable Embedding) (${z_0^0 = x_{class} }$)</strong>，该嵌入在 Transformer 编码器输出的状态/特征 $z_L^0$ 用作 图像表示  (等式 4)。无论是预训练还是微调，都有一个 分类头 (Classification Head) 附加在 $z_L^0$ 之后，从而用于图像分类。分类头在预训练时由一个 单层 MLP 实现，在微调时由 单个线性层 实现 (多层感知机与线性模型类似，区别在于 MLP 相对于 FC 层数增加且引入了非线性激活函数，例如 FC + GELU + FC 形式的 MLP)。</p>
<ul>
<li>以ViT-B/16为例，就是一个长度为768的向量，与之前从图片中生成的tokens拼接在一起，<code>Cat([1, 768], [196, 768]) -&gt; [197, 768]</code>。<strong>可学习嵌入</strong> 在训练时随机初始化，然后通过训练得到，其具体实现为：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 随机初始化</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, embed_dim))  <span class="comment"># shape = (1, 1, D)</span></span><br><span class="line"><span class="comment">### 分类头 (Classifier head)</span></span><br><span class="line">self.head = nn.Linear(self.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"><span class="comment">### 前馈过程 (Forward)</span></span><br><span class="line">B = x.shape[<span class="number">0</span>]  <span class="comment"># Batch Size</span></span><br><span class="line"><span class="comment"># 通过 可学习的线性投影 获取 Input Imgaes 的 Patch Embeddings (实现在 3.1 节)</span></span><br><span class="line">x = self.patch_embed(x)  <span class="comment"># x.shape = (B, N, D)</span></span><br><span class="line"><span class="comment"># 可学习嵌入 - 用于分类</span></span><br><span class="line">cls_tokens = self.cls_token.expand(B, -<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># shape = (B, 1, D)</span></span><br><span class="line"><span class="comment"># 按元素相加 附带 Position Embeddings</span></span><br><span class="line">x = x + self.pos_embed  <span class="comment"># shape = (B, N, D) - Python 广播机制</span></span><br><span class="line"><span class="comment"># 按通道拼接 获取 N+1 维 Embeddings</span></span><br><span class="line">x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)  <span class="comment"># shape = (B, N+1, D)</span></span><br></pre></td></tr></table></figure>
<h3 id="5-3-Position-Embeddings"><a href="#5-3-Position-Embeddings" class="headerlink" title="5.3 Position Embeddings"></a>5.3 Position Embeddings</h3><p><strong>位置嵌入</strong> ${E_{pos} \in R^{(N +1) \times D} }$  也被加入图像块嵌入，<strong>以保留输入图像块之间的空间位置信息</strong>。不同于 CNN，Transformer 需要位置嵌入来编码 patch tokens 的位置信息，这主要是由于 <strong>自注意力的扰动不变性 (Permutation-invariant)，即打乱 Sequence 中 tokens 的顺序并不会改变结果</strong>。</p>
<ul>
<li><p>相反，若不给模型提供图像块的位置信息，那么模型就需要通过图像块的语义来学习拼图，这就额外增加了学习成本。</p>
</li>
<li><p>关于Position Embedding就是原始Transformer中讲到的Positional Encoding，这里的Position Embedding采用的是一个可训练的参数（<code>1D Pos. Emb.</code>），是直接叠加在tokens上的（add），所以Position Embedding的shape要一样 。以ViT-B/16为例，刚刚拼接[class] token后shape是<code>[197, 768]</code>，那么这里的Position Embedding的shape也是[197, 768],所以加上位置编码信息之后，这个序列还是<code>[197×768]</code>。</p>
</li>
<li><p>对于Position Embedding作者也有做一系列对比试验，<strong>在源码中默认使用的是<code>1D Pos. Emb.</code></strong>，对比不使用Position Embedding准确率提升了大概3个点，和<code>2D Pos. Emb.</code>比起来没太大差别。</p>
<img src="/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/postion_embedding_experiment.png" class="">
</li>
</ul>
<h3 id="5-4-Transformer-Encoder"><a href="#5-4-Transformer-Encoder" class="headerlink" title="5.4 Transformer Encoder"></a>5.4 Transformer Encoder</h3><p><strong>Transformer 编码器</strong> 由交替的 <strong>多头自注意力层</strong> (MSA) 和 <strong>多层感知机块</strong> (MLP) 构成。在每个块前应用 <strong>层归一化 (Layer Norm)</strong>，在每个块后应用 <strong>残差连接 (Residual Connection)</strong>。</p>
<ul>
<li><strong>多头自注意力 (MSA)</strong> 就是原始的Multi-Head Attenti，首先产生：k、q、v，每一个都是197×768，这里因为做的是多头自注意力，所以其实最后的维度并不是768，假设现在使用的是VIsion Transformer的base版本，即多头使用了12个头，那么最后的维度就变成了768/12=64，也就是说这里的k、q、v变成了197×64，但是有12个头，有12个对应的k、q、v做自注意力操作，最后再将12个头的输出直接拼接起来，这样64拼接出来之后又变成了768，所以多头自注意力出来的结果经过拼接还是197×768。</li>
<li><strong>MLP Block</strong>，如图右侧所示，就是<strong>全连接+GELU激活函数+Dropout</strong>组成也非常简单，需要注意的是第一个全连接层会把输入节点个数翻4倍<code>[197, 768] -&gt; [197, 3072]</code>，第二个全连接层会还原回原节点个数<code>[197, 3072] -&gt; [197, 768]</code></li>
</ul>
<img src="/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/transformer_encoder.jpg" class="">
<h3 id="5-5-Inductive-bias"><a href="#5-5-Inductive-bias" class="headerlink" title="5.5 Inductive bias"></a>5.5 Inductive bias</h3><p><strong>归纳偏置 (Inductive bias)</strong>：注意到，Vision Transformer 的图像特定归纳偏置比 CNN 少得多。在 CNN 中，<strong>局部性 (locality)、二维邻域结构 (two-dimensional neighborhood structure,) 和 平移等效性 (translation equivariance) </strong>存在于整个模型的每一层中。<strong>而在 ViT 中，只有 MLP 层是局部和平移等变的，因为自注意力层都是全局的</strong>。二维邻域结构 的使用非常谨慎：在模型开始时通过将图像切分成块，并在微调时调整不同分辨率图像的位置嵌入 (如下所述)。此外，初始化时的位置嵌入不携带有关图像块的 2D 位置的信息，图像块之间的所有空间关系都必须从头开始学习。关于归纳偏置，详见《<strong><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_39478403/article/details/121107057">【机器学习】浅谈 归纳偏置 (Inductive Bias)_闻韶-CSDN博客_归纳偏置</a></strong>》 </p>
<h3 id="5-6Hybrid-Architecture"><a href="#5-6Hybrid-Architecture" class="headerlink" title="5.6Hybrid Architecture"></a>5.6Hybrid Architecture</h3><p>混合架构 (Hybrid Architecture)：作为原始图像块的替代方案，输入序列可由 CNN 的特征图构成。在这种混合模型中，图像块嵌入投影 被用在 经 CNN 特征提取的块 而非 原始输入图像块。作为一种特殊情况，块的空间尺寸可以为 ，这意味着输入序列是通过 简单地将特征图的空间维度展平并投影到 Transformer 维度 获得的。然后，如上所述添加了分类输入嵌入和位置嵌入，再将三者组成的整体馈入 Transformer 编码器。简单来说，就是先用 CNN 提取图像特征，然后由 CNN 提取的特征图构成图像块嵌入。由于 CNN 已经将图像降采样了，所以块尺寸可为 。</p>
<ul>
<li>既然transformer全局建模的能力比较强，卷积神经网络又比较data efficient（不需要太多的训练数据），所以搞出了一个混合的网络，前面是卷积神经网络，后面是transformer</li>
<li>假设现在将一整张图输入一个CNN，比如说Res50，最后出来一个14×14的特征图，这个特征图拉直了以后恰好也是196个元素，然后用新的到的196个元素去和全连接层做操作得到新的patch embedding。</li>
</ul>
<h2 id="六、Experiments"><a href="#六、Experiments" class="headerlink" title="六、Experiments"></a>六、Experiments</h2><p>为了了解训练好每个模型到底需要多少数据，在不同大小的数据集上做预训练，然后在很多的数据集上做测试，作者一共训练了三种模型，参数如下图所示：</p>
<img src="/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/table1.png" class="">
<p>而ViT 并不像 CNN 那样具有 Inductive Bias，若直接在 ImageNet 上训练，同 level 的 ViT 效果不如 ResNet。但若先在较大的数据集上预训练，然后再对特定的较小数据集进行微调，则效果优于 ResNet。比如 ViT 在Google 私有的 300M JFT 数据集上预训练后，在 ImageNet 上的最好的 Top-1 ACC 可达 88.55%，这在当时已和 ImageNet上的 SOTA 相当了 (Noisy Student EfficientNet-L2 效果为 88.5%，Google 最新的 SOTA 是 Meta Pseudo Labels，效果可达 90.2%)：</p>
<img src="/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/table2.png" class="">
<p>vision trasformer到底需要多少数据才能训练的比较好？下图中图三是整个 vision trasformer论文最重要的take home message，是作者最想让读者知道的，这张图基本上把所有的实验都快概括了。</p>
<img src="/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/figure3.png" class="">
<ul>
<li>图三的主要意思是说，灰色代表bit，也就是各种大小的resnet，最下面表示50，最上面表示152，他所想要展示的是在中间的灰色区域就是resnet能达到的效果范围，剩下的圆点就是各种大小不一的vision transformer。</li>
<li>在最小的ImageNet上做预训练时，vision transformer是完全不如resnet，vision transformer基本上所有的点都在灰色区域的下面。这说明vision transformer在中小型数据集上做预训练的时候的效果是远不如残差网络的，原因就是因为vision transformer没有使用先验知识（归纳偏置），所以它需要更大的数据去让网络学得更好。</li>
<li>只有当用特别大的数据集JFT-300M时，vision transformer是比bit对应的res152还要高的。</li>
</ul>
<p><strong>总之这个图所要表达的是两个信息：</strong></p>
<ol>
<li>如果想用vision transformer，那么得至少准备差不多和ImageNet-21k差不多大小的数据集，如果只有很小的数据集，还是选择使用卷积神经网络比较好。</li>
<li>当已经拥有了比ImageNet-21k更大的数据集的时候，用vision transformer就能得到更好的结果，它的扩展性更好一些。Transformer 的一个特色其 <strong>Scalability</strong>：<strong>当模型和数据量提升时，性能持续提升</strong>。在大数据下，ViT 可能会发挥更大的优势。</li>
</ol>
<p>最后作者想看一下自注意力是否起作用了，之所以想用transformer，就是因为自注意力的操作能够模拟长距离的关系。论文分析了 不同 Layers 的 Mean Attention Distance，其类比于 CNN 的感受野。结果表明：<strong>前面层的自注意力的距离虽然差异很大，有的自注意力中的头注意的距离还是挺近的，能达到20个像素，但是有的头能达到120个像素；随着网络越来越深，网络学到的特征也会变得越来越高级，越来越具有语义信息，模型后半部分自注意力的距离已经非常远了，基本覆盖全局，也就是说它已经学到了带有语义性的概念，而不是靠邻近的像素点去进行判断</strong>。</p>
<img src="/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/figure7.png" class="">
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://hr-insist.github.io">HRui</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://hr-insist.github.io/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/">https://hr-insist.github.io/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://hr-insist.github.io" target="_blank">Modesty</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a><a class="post-meta__tags" href="/tags/Computer-Vision/">Computer Vision</a></div><div class="post_share"><div class="social-share" data-image="/Paper-Notes/Paper-ICLR-2020-Vision_Transformer/ViT.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Paper-Notes/Paper-ICCV-2021-Swin_Transformer/"><img class="prev-cover" src="/Paper-Notes/Paper-ICCV-2021-Swin_Transformer/cover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">论文阅读 Swin Transformer</div></div></a></div><div class="next-post pull-right"><a href="/Hexo/%E5%85%B3%E4%BA%8E%E6%88%91%E7%9A%84Butterfly%E4%B8%BB%E9%A2%98%E7%9A%84%E6%89%80%E6%9C%89%E4%BC%98%E5%8C%96/"><img class="next-cover" src="/Hexo/%E5%85%B3%E4%BA%8E%E6%88%91%E7%9A%84Butterfly%E4%B8%BB%E9%A2%98%E7%9A%84%E6%89%80%E6%9C%89%E4%BC%98%E5%8C%96/cover.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">关于我的Butterfly主题的所有优化</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/Paper-Notes/Paper-CVPR-2021-MPRnet/" title="论文阅读 Multi-Stage Progressive Image Restoration"><img class="cover" src="/Paper-Notes/Paper-CVPR-2021-MPRnet/MPRnet.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-20</div><div class="title">论文阅读 Multi-Stage Progressive Image Restoration</div></div></a></div><div><a href="/Paper-Notes/Paper-CVPR-2021-Uformer/" title="论文笔记Uformer：A General U-Shaped Transformer for Image Restoration"><img class="cover" src="/Paper-Notes/Paper-CVPR-2021-Uformer/Uformer.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-11-30</div><div class="title">论文笔记Uformer：A General U-Shaped Transformer for Image Restoration</div></div></a></div><div><a href="/Paper-Notes/Paper-CVPR-2022-Restormer/" title="【CVPR 2022】Restormer：Efficient Transformer for High-Resolution Image Restoration"><img class="cover" src="/Paper-Notes/Paper-CVPR-2022-Restormer/cover.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-28</div><div class="title">【CVPR 2022】Restormer：Efficient Transformer for High-Resolution Image Restoration</div></div></a></div><div><a href="/Paper-Notes/Paper-ICCV-2021-Swin_Transformer/" title="论文阅读 Swin Transformer"><img class="cover" src="/Paper-Notes/Paper-ICCV-2021-Swin_Transformer/cover.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-19</div><div class="title">论文阅读 Swin Transformer</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HRui</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/HR-Insist"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81Abstract"><span class="toc-text">一、Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Introduction"><span class="toc-text">二、Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81Conclution"><span class="toc-text">三、Conclution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Related-Work"><span class="toc-text">四、Related Work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81Method"><span class="toc-text">五、Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Patch-Embeddings"><span class="toc-text">5.1 Patch Embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Learnable-Embedding"><span class="toc-text">5.2 Learnable Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-Position-Embeddings"><span class="toc-text">5.3 Position Embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-Transformer-Encoder"><span class="toc-text">5.4 Transformer Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-Inductive-bias"><span class="toc-text">5.5 Inductive bias</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-6Hybrid-Architecture"><span class="toc-text">5.6Hybrid Architecture</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81Experiments"><span class="toc-text">六、Experiments</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/Java/SpringBoot%E6%8E%A5%E5%8F%A3%E6%8E%A5%E6%94%B6%E6%95%B4%E6%95%B0%E5%9E%8B%E5%AD%97%E6%AE%B5%E4%BC%A0%E5%B0%8F%E6%95%B0%E4%B8%8D%E6%8A%A5%E9%94%99/" title="SpringBoot接口接收整数型字段传小数不报错"><img src="/Java/SpringBoot%E6%8E%A5%E5%8F%A3%E6%8E%A5%E6%94%B6%E6%95%B4%E6%95%B0%E5%9E%8B%E5%AD%97%E6%AE%B5%E4%BC%A0%E5%B0%8F%E6%95%B0%E4%B8%8D%E6%8A%A5%E9%94%99/cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SpringBoot接口接收整数型字段传小数不报错"/></a><div class="content"><a class="title" href="/Java/SpringBoot%E6%8E%A5%E5%8F%A3%E6%8E%A5%E6%94%B6%E6%95%B4%E6%95%B0%E5%9E%8B%E5%AD%97%E6%AE%B5%E4%BC%A0%E5%B0%8F%E6%95%B0%E4%B8%8D%E6%8A%A5%E9%94%99/" title="SpringBoot接口接收整数型字段传小数不报错">SpringBoot接口接收整数型字段传小数不报错</a><time datetime="2024-07-23T06:30:52.000Z" title="发表于 2024-07-23 14:30:52">2024-07-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Java/Dubbo%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C/" title="Dubbo参数校验"><img src="/Java/Dubbo%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C/cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Dubbo参数校验"/></a><div class="content"><a class="title" href="/Java/Dubbo%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C/" title="Dubbo参数校验">Dubbo参数校验</a><time datetime="2024-07-23T01:58:40.000Z" title="发表于 2024-07-23 09:58:40">2024-07-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Java/Dubbo%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/" title="Dubbo异常处理"><img src="/Java/Dubbo%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Dubbo异常处理"/></a><div class="content"><a class="title" href="/Java/Dubbo%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/" title="Dubbo异常处理">Dubbo异常处理</a><time datetime="2024-07-22T15:40:58.000Z" title="发表于 2024-07-22 23:40:58">2024-07-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By HRui</div><div class="footer_custom_text"><p> <a style="margin-inline:5px"target="_blank" href="https://hexo.io/"> <img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为 Hexo" alt="HEXO"> </a> <a style="margin-inline:5px"target="_blank" href="https://butterfly.js.org/"> <img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用 Butterfly" alt="Butterfly"> </a> <a style="margin-inline:5px"target="_blank" href="https://github.com/"> <img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由 GitHub 托管" alt="GitHub"> </a> <a style="margin-inline:5px"target="_blank"href="http://creativecommons.org/licenses/by-nc-sa/4.0/"> <img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" alt="img" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"> </a> </p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>